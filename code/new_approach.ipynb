{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57828 samples, validate on 24784 samples\n",
      "Epoch 1/1\n",
      "57828/57828 [==============================] - 109s - loss: 0.0416 - val_loss: 0.0406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a09445908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, RepeatVector,concatenate, Reshape,multiply\n",
    "from keras.models import Sequential, Model\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_emb(tokenizer):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('glove.6B.300d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((5000, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= 5000:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def emb_model(n, embedding_matrix,y):\n",
    "    input_data = Input(shape=[n])\n",
    "    embedding_layer = Embedding(5000, 300, weights=[embedding_matrix],\n",
    "                                input_length=25, trainable=False)(input_data)\n",
    "    model = Model(input_data, embedding_layer)\n",
    "    return model.predict(y)\n",
    "\n",
    "\n",
    "Y_emb = np.load('coco_pre_prop_Y_new.npy')\n",
    "X = np.load('coco_pre_prop_X_new.npy')\n",
    "X = np.reshape(X, [X.shape[0], X.shape[-1]])\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(Y_emb)\n",
    "sequences = tokenizer.texts_to_sequences(Y_emb)\n",
    "data = pad_sequences(sequences, maxlen=25,truncating='post', padding='post')\n",
    "embedding_matrix = get_emb(tokenizer)\n",
    "data_input = data[:,:-1]\n",
    "Y_input = emb_model(24,embedding_matrix, data_input)\n",
    "Y_target = emb_model(25,embedding_matrix, data)\n",
    "mask = Y_target.sum(axis = -1, keepdims = True) != 0\n",
    "mask = np.repeat(mask, 300,axis = 2)\n",
    "initializer = keras.initializers.RandomUniform(minval=-0.08, maxval=0.08)\n",
    "image_feature = Input(shape = [2048], name = 'img_input')\n",
    "dense1 = Dense(300,kernel_initializer=initializer,name = 'dense1')(image_feature)\n",
    "image_emb = Reshape((1,300), name = 'image_emb')(dense1)\n",
    "cap_input = Input(shape = [24,300], name = 'cap_input')\n",
    "merged_input = concatenate([image_emb, cap_input],axis = 1)\n",
    "lstm_cell = LSTM(300,return_sequences=True,kernel_initializer=initializer,dropout=0.3,name = 'lstm_cell')(merged_input)\n",
    "mask_input = Input(shape = [25,300], name = 'mask_input')\n",
    "final_output = multiply([lstm_cell,mask_input])\n",
    "show_and_tell = Model(inputs = [image_feature,cap_input, mask_input], outputs = [final_output])\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 2\n",
    "    drop = 0.5\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "op = optimizers.SGD(lr=0.0, decay=0.0, nesterov=False)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "history = LossHistory()\n",
    "show_and_tell.compile(optimizer = op, loss = 'mse')\n",
    "show_and_tell.fit({'img_input':X, 'cap_input':Y_input,'mask_input':mask}, Y_target,epochs = 1,validation_split=0.3, callbacks=[lrate,history])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "img_input (InputLayer)           (None, 2048)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense1 (Dense)                   (None, 300)           614700                                       \n",
      "____________________________________________________________________________________________________\n",
      "image_emb (Reshape)              (None, 1, 300)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "cap_input (InputLayer)           (None, 24, 300)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 25, 300)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_cell (LSTM)                 (None, 25, 300)       721200                                       \n",
      "____________________________________________________________________________________________________\n",
      "mask_input (InputLayer)          (None, 25, 300)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)            (None, 25, 300)       0                                            \n",
      "====================================================================================================\n",
      "Total params: 1,335,900.0\n",
      "Trainable params: 1,335,900.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "show_and_tell.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_tell.save_weights('new_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = Input((1,300))\n",
    "lstm_cell = LSTM(300,kernel_initializer=initializer,dropout=0.3,name = 'lstm_cell', return_sequences = True)(test_input)\n",
    "tell = Model(inputs = test_input, outputs = lstm_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tell.load_weights('new_model.h5',by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_cell (LSTM)             (None, 1, 300)            721200    \n",
      "=================================================================\n",
      "Total params: 721,200.0\n",
      "Trainable params: 721,200.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tell.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = show_and_tell.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = X[0].dot(w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre(img):\n",
    "    final = []\n",
    "    global w,b,embedding_matrix\n",
    "    img_out = img.dot(w) + b\n",
    "    img_out = np.reshape(img_out, newshape=[1,1,img_out.shape[0]])\n",
    "    i = 0 \n",
    "    while i < 25:\n",
    "        out = tell.predict(img_out)\n",
    "        final.append(np.argmax(np.reshape(out,newshape=[300,]).dot(embedding_matrix.T)))\n",
    "        img_out = out\n",
    "        i +=1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
